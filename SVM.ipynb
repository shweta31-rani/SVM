{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.What is a Support Vector Machine (SVM)?\n",
        "-F\n",
        "\n",
        "ANS:A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for both classification and regression tasks. It works by finding the optimal hyperplane that best separates data points into different classes, maximizing the margin between them. This margin maximization helps the SVM generalize well to new, unseen data."
      ],
      "metadata": {
        "id": "DW8jXheLXklc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: What is the difference between Hard Margin and Soft Margin SVM.-F\n",
        "\n",
        "In Support Vector Machines (SVM), both hard margin and soft margin aim to find a hyperplane that best separates data points into different classes. The key difference lies in their handling of misclassifications and linearly separable data. Hard margin SVM requires perfectly separable data and finds the widest possible margin, while soft margin SVM allows for some misclassifications to handle cases where data isn't perfectly separable or when a wide margin is desired."
      ],
      "metadata": {
        "id": "gwQGg0AJX44N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the mathematical intuition behind SVM.IF\n",
        "\n",
        "ANS:The core mathematical idea behind Support Vector Machines (SVM) is to find the optimal hyperplane that best separates data points into different classes, maximizing the margin between the hyperplane and the closest data points (support vectors). This margin acts as a buffer, improving the model's ability to generalize to unseen data and handle noisy data points."
      ],
      "metadata": {
        "id": "e_N_0zCcYJ7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 What is the role of Lagrange Multipliers in SVM?-F\n",
        "\n",
        "ANS: Support Vector Machines (SVMs), Lagrange multipliers are used to transform the problem of finding the optimal hyperplane that separates data into different classes into a more convenient form for optimization, specifically the dual problem"
      ],
      "metadata": {
        "id": "SeE3AzQLYWYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are Support Vectors in SVM?-F\n",
        "\n",
        "ANS:In Support Vector Machines (SVMs), support vectors are the data points that are closest to the decision boundary (hyperplane) separating different classes. These points are crucial because they determine the position and orientation of the hyperplane and ultimately the margin between classes"
      ],
      "metadata": {
        "id": "Y76avs5jYkaD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is a Support Vector Classifier (SVC)?-F\n",
        "\n",
        "ANS:A Support Vector Classifier (SVC) is a specific type of Support Vector Machine (SVM) used for classification tasks. It aims to find the best hyperplane that separates data points into different classes, maximizing the margin between the hyperplane and the nearest data points (support vectors). Essentially, SVC is an SVM tailored for predicting categorical outcomes."
      ],
      "metadata": {
        "id": "n7Z_XAnOYz8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is a Support Vector Regressor (SVR)?-F\n",
        "\n",
        "ANS:A Support Vector Regressor (SVR) is a machine learning algorithm used for regression tasks, which aims to predict continuous numerical values. It's an extension of the Support Vector Machine (SVM) algorithm, adapted for regression rather than classification. SVR works by finding a hyperplane that best fits the data while maximizing the margin (the distance between the hyperplane and the nearest data points)."
      ],
      "metadata": {
        "id": "o6ofIHNHZBK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Kernel Trick in SVM?-F\n",
        "\n",
        "ANS:The kernel trick in Support Vector Machines (SVMs) is a technique that allows SVMs to classify data that is not linearly separable by implicitly mapping the data into a higher-dimensional feature space where a linear separator can be used."
      ],
      "metadata": {
        "id": "orOha5ZvZS2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Compare Linear Kernel, Polynomial Kernel, and RBF Kernel?-F\n",
        "\n",
        "\n",
        "ANS:Linear, polynomial, and RBF (Radial Basis Function) kernels are fundamental components of Support Vector Machines (SVMs), each with distinct characteristics and applications. Linear kernels are simple and efficient for linearly separable data, while polynomial and RBF kernels handle non-linear data, with RBF often being the default choice due to its ability to capture complex relationships."
      ],
      "metadata": {
        "id": "xoA8FykWZhlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.:What is the effect of the C parameter in SVM?-F\n",
        "\n",
        "\n",
        "ANS:In Support Vector Machines (SVMs), the C parameter acts as a regularization parameter that controls the trade-off between maximizing the margin (the distance between the hyperplane and the nearest data points) and minimizing the number of misclassifications in the training data."
      ],
      "metadata": {
        "id": "TdHqEaYEZr0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.: What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "\n",
        "\n",
        "ANS:In an RBF (Radial Basis Function) kernel SVM, the gamma parameter controls the influence of individual training examples on the decision boundary. A small gamma value means that a training example will have a wider influence, creating a smoother, simpler decision boundary. Conversely, a large gamma value means a training example will have a more local influence, resulting in a more complex, potentially overfitted decision boundary."
      ],
      "metadata": {
        "id": "5n6vUfEfZ4zK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "ANS:The Naïve Bayes classifier is a simple probabilistic classifier that applies Bayes' theorem with the strong assumption of feature independence. It's called \"naïve\" because it assumes that the presence of one feature in a class is unrelated to the presence of any other feature. This simplification, while often unrealistic, makes the algorithm computationally efficient and surprisingly effective for many real-world problems."
      ],
      "metadata": {
        "id": "uuc0MvV5aExL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Bayes’ Theorem?\n",
        "\n",
        "\n",
        "ANS:Bayes' Theorem is a mathematical formula that describes the probability of an event based on prior knowledge of conditions related to the event."
      ],
      "metadata": {
        "id": "uzJykNrHaRHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes?\n",
        "\n",
        "\n",
        "ANS:Bernoulli Naive bayes is good at handling boolean/binary attributes, while Multinomial Naive bayes is good at handling discrete values and Gaussian naive bayes is good at handling continuous values."
      ],
      "metadata": {
        "id": "w12M83Wgab6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15: When should you use Gaussian Naïve Bayes over other variants?\n",
        "\n",
        "\n",
        "ANS:Gaussian Naive Bayes is best used when dealing with datasets containing continuous numerical features that are roughly normally distributed. This means the data points tend to cluster around a mean value, forming a bell curve shape. If your features are not continuous or don't follow a normal distribution, other Naive Bayes variants like Multinomial Naive Bayes (for discrete features) or Bernoulli Naive Bayes (for binary features) might be more appropriate."
      ],
      "metadata": {
        "id": "yts6JT12auf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the key assumptions made by Naïve Bayes?\n",
        "\n",
        "\n",
        "\n",
        "ANS:The basic assumption in Naïve Bayes is one of conditional independence between all independent variable features. Conditional independence ensures that how one feature affects an outcome in no way interacts with how another variable affects the same outcome."
      ],
      "metadata": {
        "id": "KRRDJlv-cXvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17: What are the advantages and disadvantages of Naïve Bayes?\n",
        "\n",
        "\n",
        "ANS:Naive Bayes classifiers offer advantages like simplicity, speed, and efficiency with large datasets, especially in text-based applications. However, they are based on the strong assumption of feature independence, which rarely holds true in real-world scenarios, potentially impacting accuracy."
      ],
      "metadata": {
        "id": "Lfdb93kmcufy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18 Why is Naïve Bayes a good choice for text classification?\n",
        "\n",
        "\n",
        "ANS:Naïve Bayes is a popular choice for text classification due to its simplicity, speed, and effectiveness with high-dimensional data like text. Its ability to handle large datasets and its relatively low computational cost make it well-suited for tasks like spam filtering, sentiment analysis, and document categorization."
      ],
      "metadata": {
        "id": "NSP3qkZLdKmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19 Compare SVM and Naïve Bayes for classification tasks.\n",
        "\n",
        "\n",
        "ANS:SVM and Naive Bayes are both popular classification algorithms, but they differ significantly in their approaches and performance characteristics. SVM, with its focus on finding the optimal hyperplane, generally outperforms Naive Bayes in complex, non-linearly separable datasets, especially when dealing with interactions between features. However, Naive Bayes excels in speed and simplicity, making it a good choice for large datasets or when computational resources are limited."
      ],
      "metadata": {
        "id": "hBVVq3cOdhiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How does Laplace Smoothing help in Naïve Bayes?\n",
        "\n",
        "ANS:Laplace smoothing, also known as add-one smoothing, is a technique used in Naive Bayes to prevent zero probabilities, which can lead to inaccurate classifications. It works by adding a small value (usually 1) to all the frequency counts in the training data, ensuring that even unseen features have a non-zero probability. This prevents the model from completely disregarding an instance during classification simply because it contains a word not encountered during training."
      ],
      "metadata": {
        "id": "piwQRRSGd1uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#: Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy\n",
        "# 1. Import necessary libraries\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# 2. Load the Iris dataset\n",
        "iris = sns.load_dataset(\"iris\")\n",
        "X = iris.iloc[:, :-1].values  # features: sepal/petal measurements\n",
        "y = iris['species'].values    # labels: setosa, versicolor, virginica\n",
        "\n",
        "# 3. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=0, stratify=y\n",
        ")\n",
        "\n",
        "# 4. Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 5. Initialize and train the SVM classifier\n",
        "svm = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# 6. Make predictions on the test set\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# 7. Evaluation\n",
        "print(\"Training accuracy: {:.3f}\".format(accuracy_score(y_train, svm.predict(X_train))))\n",
        "print(\"Test accuracy:     {:.3f}\".format(accuracy_score(y_test, y_pred)))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=iris['species'].unique()))\n",
        "\n",
        "# 8. (Optional) Cross-validation for a more robust estimate\n",
        "cv_scores = cross_val_score(svm, X, y, cv=5)\n",
        "print(\"5‑fold cross‑validation accuracy: {:.3f} ± {:.3f}\".format(cv_scores.mean(), cv_scores.std()))\n"
      ],
      "metadata": {
        "id": "NOHY78h5eFdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 2. Load dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 4. Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 5. Initialize SVM models\n",
        "svm_lin = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
        "\n",
        "# 6. Train\n",
        "svm_lin.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# 7. Predict\n",
        "y_pred_lin = svm_lin.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# 8. Evaluate\n",
        "acc_lin = accuracy_score(y_test, y_pred_lin)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"Linear SVM Test Accuracy: {acc_lin:.4f}\")\n",
        "print(f\"RBF SVM Test Accuracy:    {acc_rbf:.4f}\\n\")\n",
        "\n",
        "print(\"Linear Kernel Metrics:\")\n",
        "print(confusion_matrix(y_test, y_pred_lin))\n",
        "print(classification_report(y_test, y_pred_lin))\n",
        "\n",
        "print(\"RBF Kernel Metrics:\")\n",
        "print(confusion_matrix(y_test, y_pred_rbf))\n",
        "print(classification_report(y_test, y_pred_rbf))\n",
        "\n",
        "# 9. (Optional) Cross-validation comparison\n",
        "cv_lin = cross_val_score(svm_lin, X, y, cv=5)\n",
        "cv_rbf = cross_val_score(svm_rbf, X, y, cv=5)\n",
        "print(f\"5‑fold CV Accuracy: Linear={cv_lin.mean():.4f}±{cv_lin.std():.4f}, RBF={cv_rbf.mean():.4f}±{cv_rbf.std():.4f}\")\n"
      ],
      "metadata": {
        "id": "tEpFhxyeeYIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# 2. Load data\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# 3. Train–test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 5. Initialize SVR model (RBF kernel by default)\n",
        "svr = SVR(kernel='rbf', C=10.0, gamma='scale', epsilon=0.1)\n",
        "\n",
        "# 6. Train model\n",
        "svr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 7. Predict on test data\n",
        "y_pred = svr.predict(X_test_scaled)\n",
        "\n",
        "# 8. Evaluate\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error (test set): {mse:.4f}\")\n",
        "print(f\"R² score (test set): {r2:.4f}\")\n",
        "\n",
        "# 9. Optional: Hyperparameter tuning via GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [1.0, 10.0, 100.0],\n",
        "    'epsilon': [0.01, 0.1, 0.5],\n",
        "    'kernel': ['rbf', 'linear'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "grid = GridSearchCV(SVR(), param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "best = grid.best_estimator_\n",
        "y_pred_best = best.predict(X_test_scaled)\n",
        "mse_best = mean_squared_error(y_test, y_pred_best)\n",
        "r2_best = r2_score(y_test, y_pred_best)\n",
        "\n",
        "print(\"\\nBest parameters found:\", grid.best_params_)\n",
        "print(f\"Tuned SVR Mean Squared Error: {mse_best:.4f}\")\n",
        "print(f\"Tuned SVR R² score: {r2_best:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtaSQnd1ehUF",
        "outputId": "8c7afb93-ab34-4a01-a312-cdd5c599d9d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (test set): 0.3237\n",
            "R² score (test set): 0.7530\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets, svm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "# 1️⃣ Load dataset & select 2D features\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]       # use only the first two features for plotting\n",
        "y = iris.target\n",
        "\n",
        "# 2️⃣ Build an SVM pipeline with polynomial kernel\n",
        "poly_svm = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    svm.SVC(kernel='poly', degree=3, gamma='auto', coef0=1, C=1)\n",
        ")\n",
        "poly_svm.fit(X, y)\n",
        "\n",
        "# 3️⃣ Plot decision boundary for polynomial SVM\n",
        "disp = DecisionBoundaryDisplay.from_estimator(\n",
        "    poly_svm, X, response_method=\"predict\",\n",
        "    grid_resolution=200,\n",
        "    cmap=plt.cm.coolwarm, alpha=0.8\n",
        ")\n",
        "\n",
        "# 4️⃣ Overlay data points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "plt.xlabel(iris.feature_names[0])\n",
        "plt.ylabel(iris.feature_names[1])\n",
        "plt.title(\"SVM with Polynomial Kernel (degree 3)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vF5uxLJzerJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 2. Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target  # target: 0=malignant, 1=benign\n",
        "\n",
        "# 3. Split into train and test sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 4. Feature scaling (recommended but not strictly required)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 5. Initialize and train the Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predict on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# 7. Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test-set Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=data.target_names,\n",
        "            yticklabels=data.target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix - GaussianNB\")\n",
        "plt.show()\n",
        "\n",
        "# Classification report (precision, recall, F1-score)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "id": "v3A0PYZAfIkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 2. Load a subset of categories (optional)\n",
        "categories = ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers','footers','quotes'))\n",
        "\n",
        "# 3. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    newsgroups.data, newsgroups.target,\n",
        "    test_size=0.2, random_state=42, stratify=newsgroups.target\n",
        ")\n",
        "\n",
        "# 4. Build a pipeline for feature extraction (count → TF-IDF) and model\n",
        "from sklearn.pipeline import Pipeline\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB(alpha=1.0)),\n",
        "])\n",
        "\n",
        "# 5. Train the classifier\n",
        "text_clf.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predict and evaluate\n",
        "y_pred = text_clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test-set Accuracy: {acc:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=newsgroups.target_names))\n",
        "\n",
        "# 7. Optional: Plot confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=newsgroups.target_names,\n",
        "            yticklabels=newsgroups.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix – MultinomialNB')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "5is5OWUNfxqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Load the Iris dataset (only the first two features for easy visualization)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]\n",
        "y = iris.target\n",
        "\n",
        "# Define different C (regularization) values to compare\n",
        "C_values = [0.1, 1, 10, 100]\n",
        "\n",
        "# Generate meshgrid for plotting\n",
        "h = 0.02\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "# Plot setup\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, C in enumerate(C_values):\n",
        "    clf = make_pipeline(StandardScaler(),\n",
        "                        svm.SVC(kernel='linear', C=C))\n",
        "    clf.fit(X, y)\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "    ax = axes[idx]\n",
        "    ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.6)\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    ax.set_title(f\"C = {C}\")\n",
        "    ax.set_xlabel(iris.feature_names[0])\n",
        "    ax.set_ylabel(iris.feature_names[1])\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vQ4_h3smgDKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 2. Create a synthetic binary dataset\n",
        "X = np.array([\n",
        "    [1, 0, 1],\n",
        "    [0, 1, 0],\n",
        "    [1, 1, 1],\n",
        "    [0, 0, 0],\n",
        "    [1, 0, 0],\n",
        "    [0, 1, 1],\n",
        "    [1, 1, 0],\n",
        "    [0, 0, 1]\n",
        "])\n",
        "y = np.array([1, 0, 1, 0, 1, 0, 1, 0])  # binary target labels\n",
        "\n",
        "# 3. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 4. Initialize and train BernoulliNB\n",
        "bnb = BernoulliNB(alpha=1.0, binarize=None)  # default smoothing\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predict and evaluate\n",
        "y_pred = bnb.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "param_grid = {'alpha': [0.5, 1.0, 2.0], 'binarize': [0.0, 0.5, None]}\n",
        "grid = GridSearchCV(BernoulliNB(), param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "best = grid.best_estimator_\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "y_pred_best = best.predict(X_test)\n",
        "print(f\"Tuned Accuracy: {accuracy_score(y_test, y_pred_best):.4f}\")\n"
      ],
      "metadata": {
        "id": "d-IS7ogogQ8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 2. Load data & split\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Train & evaluate without scaling\n",
        "svm_no_scaling = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
        "svm_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no = svm_no_scaling.predict(X_test)\n",
        "acc_no = accuracy_score(y_test, y_pred_no)\n",
        "\n",
        "# 4. Apply scaling: fit scaler on training data only\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 5. Train & evaluate with scaling\n",
        "svm_scaled = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# 6. Print results\n",
        "print(f\"Test accuracy without scaling: {acc_no:.4f}\")\n",
        "print(f\"Test accuracy with scaling:    {acc_scaled:.4f}\")\n"
      ],
      "metadata": {
        "id": "uZPWxZUYggA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 2. Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X = X[y != 2]  # Use only two classes for binary classification\n",
        "y = y[y != 2]\n",
        "\n",
        "# 3. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 4. Train Gaussian Naïve Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred_gnb = gnb.predict(X_test)\n",
        "acc_gnb = accuracy_score(y_test, y_pred_gnb)\n",
        "\n",
        "# 5. Train Multinomial Naïve Bayes with Laplace smoothing\n",
        "# Note: MultinomialNB is typically used for discrete features\n",
        "# Here, we simulate discrete-like data by adding a small constant\n",
        "X_train_mnb = X_train + 1e-9\n",
        "X_test_mnb = X_test + 1e-9\n",
        "mnb = MultinomialNB(alpha=1.0)  # Laplace smoothing with alpha=1\n",
        "mnb.fit(X_train_mnb, y_train)\n",
        "y_pred_mnb = mnb.predict(X_test_mnb)\n",
        "acc_mnb = accuracy_score(y_test, y_pred_mnb)\n",
        "\n",
        "# 6. Display results\n",
        "print(f\"Gaussian Naïve Bayes Accuracy: {acc_gnb:.4f}\")\n",
        "print(f\"Multinomial Naïve Bayes (Laplace smoothed) Accuracy: {acc_mnb:.4f}\")\n"
      ],
      "metadata": {
        "id": "cBa49g4jgsZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 2. Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 3. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 4. Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': [0.1, 1, 'scale', 'auto'],\n",
        "    'kernel': ['linear', 'rbf', 'poly']\n",
        "}\n",
        "\n",
        "# 5. Initialize SVM classifier\n",
        "svm = SVC()\n",
        "\n",
        "# 6. Set up GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=svm, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2\n",
        ")\n",
        "\n",
        "# 7. Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 8. Best hyperparameters and accuracy\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-validation Accuracy: {:.2f}%\".format(grid_search.best_score_ * 100))\n",
        "\n",
        "# 9. Evaluate on test set\n",
        "best_svm = grid_search.best_estimator_\n",
        "y_pred = best_svm.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))\n"
      ],
      "metadata": {
        "id": "y3dwZZ2Wg0w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# 2. Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 3. Create an imbalanced dataset by removing some samples from the minority class\n",
        "X, y = X[y != 2], y[y != 2]  # Keep only classes 0 and 1\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# 4. Train SVM without class weighting\n",
        "svm_no_weight = SVC(kernel='linear', random_state=42)\n",
        "svm_no_weight.fit(X_train, y_train)\n",
        "y_pred_no_weight = svm_no_weight.predict(X_test)\n",
        "\n",
        "# 5. Train SVM with class weighting\n",
        "svm_with_weight = SVC(kernel='linear', class_weight='balanced', random_state=42)\n",
        "svm_with_weight.fit(X_train, y_train)\n",
        "y_pred_with_weight = svm_with_weight.predict(X_test)\n",
        "\n",
        "# 6. Evaluate both models\n",
        "print(\"SVM without class weighting:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_no_weight))\n",
        "print(classification_report(y_test, y_pred_no_weight))\n",
        "\n",
        "print(\"\\nSVM with class weighting:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_with_weight))\n",
        "print(classification_report(y_test, y_pred_with_weight))\n"
      ],
      "metadata": {
        "id": "CGIZ6QZfhMmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Define paths to the dataset\n",
        "ham_dir = 'path_to_easy_ham'\n",
        "spam_dir = 'path_to_spam'\n",
        "\n",
        "# Function to read emails from a directory\n",
        "def read_emails_from_dir(directory, label):\n",
        "    emails = []\n",
        "    for filename in os.listdir(directory):\n",
        "        with open(os.path.join(directory, filename), 'r', encoding='latin-1') as file:\n",
        "            emails.append(file.read())\n",
        "    return pd.DataFrame({'text': emails, 'label': [label] * len(emails)})\n",
        "\n",
        "# Load ham and spam emails\n",
        "ham_emails = read_emails_from_dir(ham_dir, 'ham')\n",
        "spam_emails = read_emails_from_dir(spam_dir, 'spam')\n",
        "\n",
        "# Combine ham and spam emails into a single DataFrame\n",
        "emails_df = pd.concat([ham_emails, spam_emails], ignore_index=True)\n",
        "\n",
        "# Shuffle the dataset\n",
        "emails_df = emails_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Preprocess the text data\n",
        "emails_df['text'] = emails_df['text'].apply(lambda x: re.sub(r'\\W', ' ', x.lower()))\n",
        "\n",
        "# Split the dataset into features and labels\n",
        "X = emails_df['text']\n",
        "y = emails_df['label']\n",
        "\n",
        "# Convert text data to numerical features using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Naïve Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "-gzT0D3QhXIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "# Load dataset (replace with your dataset path)\n",
        "df = pd.read_csv('path_to_your_dataset.csv')\n",
        "\n",
        "# Preprocessing: Convert text to lowercase\n",
        "df['text'] = df['text'].str.lower()\n",
        "\n",
        "# Split dataset into features and target variable\n",
        "X = df['text']\n",
        "y = df['label']\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "# Initialize and train SVM classifier\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "svm_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions\n",
        "svm_predictions = svm_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate performance\n",
        "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
        "print(\"SVM Accuracy:\", svm_accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, svm_predictions))\n",
        "# Initialize and train Naïve Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions\n",
        "nb_predictions = nb_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate performance\n",
        "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
        "print(\"Naïve Bayes Accuracy:\", nb_accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, nb_predictions))\n",
        "\n"
      ],
      "metadata": {
        "id": "ha0lt6tOhfFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Convert text data to lowercase\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "# Select top 1000 features using chi-square test\n",
        "selector = SelectKBest(chi2, k=1000)\n",
        "X_train_selected = selector.fit_transform(X_train_tfidf, y_train)\n",
        "X_test_selected = selector.transform(X_test_tfidf)\n",
        "# Initialize and train Naïve Bayes classifier\n",
        "nb_classifier_all = MultinomialNB()\n",
        "nb_classifier_all.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_all = nb_classifier_all.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy_all = accuracy_score(y_test, y_pred_all)\n",
        "print(\"Naïve Bayes with All Features Accuracy:\", accuracy_all)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_all))\n",
        "# Initialize and train Naïve Bayes classifier\n",
        "nb_classifier_selected = MultinomialNB()\n",
        "nb_classifier_selected.fit(X_train_selected, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_selected = nb_classifier_selected.predict(X_test_selected)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy_selected = accuracy_score(y_test, y_pred_selected)\n",
        "print(\"Naïve Bayes with Selected Features Accuracy:\", accuracy_selected)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_selected))\n"
      ],
      "metadata": {
        "id": "iU1h4M1vh-Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# Initialize and train One-vs-Rest SVM classifier\n",
        "svm_ovr = OneVsRestClassifier(SVC(kernel='linear', random_state=42))\n",
        "svm_ovr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_ovr = svm_ovr.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(\"One-vs-Rest SVM Accuracy:\", accuracy_ovr)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_ovr))\n",
        "# Initialize and train One-vs-One SVM classifier\n",
        "svm_ovo = OneVsOneClassifier(SVC(kernel='linear', random_state=42))\n",
        "svm_ovo.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_ovo = svm_ovo.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "print(\"One-vs-One SVM Accuracy:\", accuracy_ovo)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_ovo))\n",
        "print(f\"Accuracy with One-vs-Rest: {accuracy_ovr * 100:.2f}%\")\n",
        "print(f\"Accuracy with One-vs-One: {accuracy_ovo * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "s0JVumRjiO27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# Initialize and train SVM classifier with linear kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_linear = svm_linear.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "print(\"Linear Kernel SVM Accuracy:\", accuracy_linear)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_linear))\n",
        "# Initialize and train SVM classifier with polynomial kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, random_state=42)\n",
        "svm_poly.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_poly = svm_poly.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy_poly = accuracy_score(y_test, y_pred_poly)\n",
        "print(\"Polynomial Kernel SVM Accuracy:\", accuracy_poly)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_poly))\n",
        "# Initialize and train SVM classifier with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "print(\"RBF Kernel SVM Accuracy:\", accuracy_rbf)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rbf))\n"
      ],
      "metadata": {
        "id": "BHFENQD8irEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the\n",
        "#average accuracy\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "# Initialize Stratified K-Fold with 5 splits\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# List to store accuracy scores\n",
        "accuracies = []\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "for train_index, test_index in skf.split(X_scaled, y):\n",
        "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Initialize and train SVM classifier with RBF kernel\n",
        "    svm = SVC(kernel='rbf', random_state=42)\n",
        "    svm.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = svm.predict(X_test)\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Compute average accuracy\n",
        "average_accuracy = np.mean(accuracies)\n",
        "print(f\"Average Accuracy: {average_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "PaZUjP-Vi8pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "def evaluate_naive_bayes_with_priors(priors):\n",
        "    # Initialize Gaussian Naïve Bayes classifier with custom priors\n",
        "    model = GaussianNB(priors=priors)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "# Define different prior probabilities\n",
        "priors_list = [\n",
        "    [0.5, 0.5],  # Equal priors\n",
        "    [0.3, 0.7],  # Skewed priors\n",
        "    [0.7, 0.3]   # Skewed priors\n",
        "]\n",
        "\n",
        "# Evaluate performance for each set of priors\n",
        "for priors in priors_list:\n",
        "    accuracy = evaluate_naive_bayes_with_priors(priors)\n",
        "    print(f\"Priors: {priors} => Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "cQIRgEwBjLxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# Initialize and train SVM classifier with RBF kernel\n",
        "svm_no_rfe = SVC(kernel='rbf', random_state=42)\n",
        "svm_no_rfe.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_no_rfe = svm_no_rfe.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy_no_rfe = accuracy_score(y_test, y_pred_no_rfe)\n",
        "print(f\"Accuracy without RFE: {accuracy_no_rfe * 100:.2f}%\")\n",
        "# Initialize SVM classifier with RBF kernel\n",
        "svm = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Initialize RFE with SVM estimator and select top 10 features\n",
        "rfe = RFE(estimator=svm, n_features_to_select=10)\n",
        "rfe.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Select the features identified by RFE\n",
        "X_train_rfe = rfe.transform(X_train_scaled)\n",
        "X_test_rfe = rfe.transform(X_test_scaled)\n",
        "\n",
        "# Train SVM classifier on selected features\n",
        "svm_rfe = SVC(kernel='rbf', random_state=42)\n",
        "svm_rfe.fit(X_train_rfe, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rfe = svm_rfe.predict(X_test_rfe)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "print(f\"Accuracy with RFE: {accuracy_rfe * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "45x8t1WOj66J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        " Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and\n",
        "F1-Score instead of accuracy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# Initialize and train SVM classifier with RBF kernel\n",
        "svm_no_rfe = SVC(kernel='rbf', random_state=42)\n",
        "svm_no_rfe.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_no_rfe = svm_no_rfe.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy_no_rfe = accuracy_score(y_test, y_pred_no_rfe)\n",
        "print(f\"Accuracy without RFE: {accuracy_no_rfe * 100:.2f}%\")\n",
        "# Initialize SVM classifier with RBF kernel\n",
        "svm = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Initialize RFE with SVM estimator and select top 10 features\n",
        "rfe = RFE(estimator=svm, n_features_to_select=10)\n",
        "rfe.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Select the features identified by RFE\n",
        "X_train_rfe = rfe.transform(X_train_scaled)\n",
        "X_test_rfe = rfe.transform(X_test_scaled)\n",
        "\n",
        "# Train SVM classifier on selected features\n",
        "svm_rfe = SVC(kernel='rbf', random_state=42)\n",
        "svm_rfe.fit(X_train_rfe, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rfe = svm_rfe.predict(X_test_rfe)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "print(f\"Accuracy with RFE: {accuracy_rfe * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "-Q8R5RsdkNf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# Initialize and train Naïve Bayes classifier\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_proba = nb.predict_proba(X_test_scaled)\n",
        "\n",
        "# Calculate Log Loss\n",
        "loss = log_loss(y_test, y_pred_proba)\n",
        "print(f\"Log Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "PP_RFOM9kgrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# Initialize and train SVM classifier with RBF kernel\n",
        "svm = SVC(kernel='rbf', random_state=42)\n",
        "svm.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm.predict(X_test_scaled)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix using Seaborn\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Malignant', 'Benign'],\n",
        "            yticklabels=['Malignant', 'Benign'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix for SVM Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VyQ-pGvck1w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_boston  # or fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load a housing dataset (for example purposes)\n",
        "data = load_boston()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 1. Without scaling\n",
        "svr = SVR(kernel='rbf', C=1.0, gamma='scale')\n",
        "svr.fit(X_train, y_train)\n",
        "y_pred_no_scaling = svr.predict(X_test)\n",
        "mae_no = mean_absolute_error(y_test, y_pred_no_scaling)\n",
        "print(f\"MAE without scaling: {mae_no:.3f}\")\n",
        "\n",
        "# 2. With feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "svr_scaled = SVR(kernel='rbf', C=1.0, gamma='scale')\n",
        "svr_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = svr_scaled.predict(X_test_scaled)\n",
        "mae_scaled = mean_absolute_error(y_test, y_pred_scaled)\n",
        "print(f\"MAE with scaling:    {mae_scaled:.3f}\")\n"
      ],
      "metadata": {
        "id": "R7o2wYM_lI4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC\n",
        "\n",
        "#score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, RocCurveDisplay\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target  # binary target (0/1)\n",
        "\n",
        "# 2. Split train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Scale features\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Train Gaussian Naïve Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 5. Predict probabilities for ROC-AUC\n",
        "y_probs = gnb.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# 6. Compute ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
        "auc_score = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "print(f\"ROC AUC Score: {auc_score:.4f}\")\n",
        "\n",
        "# 7. Plot ROC curve\n",
        "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc_score, estimator_name=\"GaussianNB\").plot()\n",
        "plt.title(\"ROC Curve for Gaussian Naïve Bayes\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CStCsszClRz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve, PrecisionRecallDisplay\n",
        "\n",
        "# Load binary classification dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[iris.target < 2]\n",
        "y = iris.target[iris.target < 2]\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.5, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Build pipeline: scaling + SVM with probability estimates enabled\n",
        "svm_clf = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    SVC(kernel='linear', probability=True, random_state=42)\n",
        ")\n",
        "\n",
        "# Train classifier\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Compute decision scores or probabilities\n",
        "y_scores = svm_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute average precision (area under PR curve)\n",
        "avg_prec = average_precision_score(y_test, y_scores)\n",
        "\n",
        "print(f\"Average precision‑recall score = {avg_prec:.2f}\")\n",
        "\n",
        "# Compute precision and recall for all thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Plot manually (optional)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.step(recall, precision, where=\"post\", color=\"b\", alpha=0.5)\n",
        "plt.fill_between(recall, precision, step=\"post\", alpha=0.2, color=\"b\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title(f\"Precision‑Recall curve: AP={avg_prec:.2f}\")\n",
        "plt.show()\n",
        "\n",
        "# Or use built‑in display API\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall, average_precision=avg_prec)\n",
        "disp.plot()\n",
        "plt.title(\"Precision‑Recall curve (via PrecisionRecallDisplay)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ObHeFLhwlkVk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}